{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000/100/10 neural network\n",
    "import torch\n",
    "\n",
    "n = 64 # Training set size\n",
    "d_in = 1000 # Dimension of data in\n",
    "h = 100 # Dimension of hiddden layer\n",
    "d_out = 10 # Dimension of data out\n",
    "epochs = 501 # Number of epochs used to train network\n",
    "\n",
    "x = torch.randn(n, d_in, dtype = torch.float, requires_grad = False) # Input tensor\n",
    "y = torch.randn(n, d_out, dtype = torch.float, requires_grad = False)# Output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:    0 - Loss: 3.43e+07\n",
      "Iteration:   50 - Loss: 1.38e+04\n",
      "Iteration:  100 - Loss: 4.88e+02\n",
      "Iteration:  150 - Loss: 3.09e+01\n",
      "Iteration:  200 - Loss: 2.45e+00\n",
      "Iteration:  250 - Loss: 2.18e-01\n",
      "Iteration:  300 - Loss: 2.07e-02\n",
      "Iteration:  350 - Loss: 2.14e-03\n",
      "Iteration:  400 - Loss: 3.11e-04\n",
      "Iteration:  450 - Loss: 7.98e-05\n",
      "Iteration:  500 - Loss: 3.17e-05\n"
     ]
    }
   ],
   "source": [
    "# Weights to connect layers\n",
    "w1 = torch.randn(d_in, h) # 1000 x 100 weights connects 1000 neurons from input layer to 100 neurons on hidden layer\n",
    "w2 = torch.randn(h, d_out) # 100 x 10 weights connects 100 neurons from hidden layer to 10 neurons on output layer\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass to compute a predicted y\n",
    "    hidden = x.mm(w1) # Matrix multiply by existing weights to create hidden layer\n",
    "    hidden_relu = hidden.clamp(min = 0) # Apply activation function (everything < 0 = 0)\n",
    "    y_predict = hidden_relu.mm(w2) # Matrix multiply the activation function by the second set of weights\n",
    "    \n",
    "    loss = (y_predict - y).pow(2).sum() # Calculate the MSE Loss\n",
    "    if not epoch % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.2e\" % (epoch, loss.item()))\n",
    "    \n",
    "    # Adjust the weights with respect to loss (backward pass)\n",
    "    grad_y_predict = 2.0 * (y_predict - y) # Vectorized difference between prediction against desired values\n",
    "    w2_grad = hidden_relu.t().mm(grad_y_predict) # Second level weights, use a hidden relu function, transpose, then matrix multiply\n",
    "    grad_h_relu = grad_y_predict.mm(w2.t()) # Do the same for level 1 weights (multiply by transposed second set weights)\n",
    "    grad_h = grad_h_relu.clone() # Clone to not modify values\n",
    "    grad_h[h < 0] = 0 # Compute gradient h (same as '.clamp(min = 0)')\n",
    "    w1_grad = x.t().mm(grad_h) # Compute the second set of weights\n",
    "    \n",
    "    # Adjust the weights with gradient descent\n",
    "    w1 -= learning_rate * w1_grad\n",
    "    w2 -= learning_rate * w2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:    0 - Loss: 3.74e+07\n",
      "Iteration:   50 - Loss: 1.07e+04\n",
      "Iteration:  100 - Loss: 2.18e+02\n",
      "Iteration:  150 - Loss: 7.44e+00\n",
      "Iteration:  200 - Loss: 3.12e-01\n",
      "Iteration:  250 - Loss: 1.46e-02\n",
      "Iteration:  300 - Loss: 9.63e-04\n",
      "Iteration:  350 - Loss: 1.56e-04\n",
      "Iteration:  400 - Loss: 5.18e-05\n",
      "Iteration:  450 - Loss: 2.52e-05\n",
      "Iteration:  500 - Loss: 1.57e-05\n"
     ]
    }
   ],
   "source": [
    "# Same process, now using Autograd\n",
    "w1 = torch.randn(d_in, h, dtype = torch.float, requires_grad = True)\n",
    "w2 = torch.randn(h, d_out, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    ## Matrix multiply the first set of weights with x data, apply an activation function to get the relu data, \n",
    "    ## then matrix multiply that by the second set of weights to get y pred.\n",
    "    y_predict = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    loss = (y_predict - y).pow(2).sum() # Calculate the MSE Loss\n",
    "    if not epoch % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.2e\" % (epoch, loss.item()))\n",
    "    \n",
    "    loss.backward() # Autograd to compute a backward pass\n",
    "    \n",
    "    # Update the weights using gradient descent with no autograd since we do not have to keep the gradients on the weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
